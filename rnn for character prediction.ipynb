{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of project_sample_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnl2l4bVsjKG",
        "colab_type": "code",
        "outputId": "1e082bef-b8bf-4fc0-ff4f-404b408b37ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLhXE53P_uM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"alice.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrHvgjUQhx85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEH9pv_0i-YD",
        "colab_type": "code",
        "outputId": "595da645-7b30-4399-991a-d779fc3c1b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "  n_chars = len(raw_text)\n",
        "  n_vocab = len(chars)\n",
        "  print(\"Total Characters: \", n_chars)\n",
        "  print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  144139\n",
            "Total Vocab:  44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD0Zumm2jDia",
        "colab_type": "code",
        "outputId": "f56d0989-ebde-4417-a323-166ed036b222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "# range(start_vaue, end_value, step)\n",
        "for i in range(0, n_chars - seq_length, 1):  \n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  144039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkLa8ay8jpNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBX2zRp6Xtbu",
        "colab_type": "code",
        "outputId": "65852fcd-db9a-4737-ff70-fa5eb9b7af53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        }
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]),return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0713 04:53:46.558779 140385624586112 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0713 04:53:46.609258 140385624586112 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0713 04:53:46.622010 140385624586112 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0713 04:53:47.001658 140385624586112 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0713 04:53:47.015373 140385624586112 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 43)                11051     \n",
            "=================================================================\n",
            "Total params: 800,555\n",
            "Trainable params: 800,555\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciowVaYoX4e7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"/content/drive/My Drive/Colab_Models/alice_in_wonderland_improved_6.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFLtcbaPX_0R",
        "colab_type": "code",
        "outputId": "286b7e88-61cf-4876-c596-db0413ff4320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "model.fit(X, y, epochs=10, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0712 06:39:34.516344 140282880669568 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "144039/144039 [==============================] - 168s 1ms/step - loss: 1.0768\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.07678, saving model to /content/drive/My Drive/Colab_Models/alice_in_wonderland_improved_6.hdf5\n",
            "Epoch 2/10\n",
            "144039/144039 [==============================] - 166s 1ms/step - loss: 1.0705\n",
            "\n",
            "Epoch 00002: loss improved from 1.07678 to 1.07054, saving model to /content/drive/My Drive/Colab_Models/alice_in_wonderland_improved_6.hdf5\n",
            "Epoch 3/10\n",
            "144039/144039 [==============================] - 165s 1ms/step - loss: 1.0725\n",
            "\n",
            "Epoch 00003: loss did not improve from 1.07054\n",
            "Epoch 4/10\n",
            "144039/144039 [==============================] - 166s 1ms/step - loss: 1.0655\n",
            "\n",
            "Epoch 00004: loss improved from 1.07054 to 1.06549, saving model to /content/drive/My Drive/Colab_Models/alice_in_wonderland_improved_6.hdf5\n",
            "Epoch 5/10\n",
            "144039/144039 [==============================] - 167s 1ms/step - loss: 1.0689\n",
            "\n",
            "Epoch 00005: loss did not improve from 1.06549\n",
            "Epoch 6/10\n",
            "144039/144039 [==============================] - 166s 1ms/step - loss: 1.0593\n",
            "\n",
            "Epoch 00006: loss improved from 1.06549 to 1.05927, saving model to /content/drive/My Drive/Colab_Models/alice_in_wonderland_improved_6.hdf5\n",
            "Epoch 7/10\n",
            "144039/144039 [==============================] - 167s 1ms/step - loss: 1.0621\n",
            "\n",
            "Epoch 00007: loss did not improve from 1.05927\n",
            "Epoch 8/10\n",
            "144039/144039 [==============================] - 168s 1ms/step - loss: 1.0612\n",
            "\n",
            "Epoch 00008: loss did not improve from 1.05927\n",
            "Epoch 9/10\n",
            "144039/144039 [==============================] - 168s 1ms/step - loss: 1.0531\n",
            "\n",
            "Epoch 00009: loss improved from 1.05927 to 1.05315, saving model to /content/drive/My Drive/Colab_Models/alice_in_wonderland_improved_6.hdf5\n",
            "Epoch 10/10\n",
            "144039/144039 [==============================] - 168s 1ms/step - loss: 1.0585\n",
            "\n",
            "Epoch 00010: loss did not improve from 1.05315\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f96056d4898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfJaf0TRdzJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50g8q_o6e5ZV",
        "colab_type": "code",
        "outputId": "1c31f2d4-17ff-4271-df23-f993c3404cf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "import sys\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"\\n=========================================Seed Text======================================================\\n\")\n",
        "str = \"\"\n",
        "print(str.join([int_to_char[value] for value in pattern]))\n",
        "print(\"\\n========================================Predicted Text=================================================== \\n\")\n",
        "\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(n_vocab)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  sys.stdout.write(result)\n",
        "  if result == \",\" or result == \".\":\n",
        "    sys.stdout.write(\"\\n\")\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print(\"\\n\\nDone.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=========================================Seed Text======================================================\n",
            "\n",
            "m mad.'\n",
            "\n",
            "'i call it purring, not growling,' said alice.\n",
            "\n",
            "'call it what you like,' said the cat. 'do \n",
            "\n",
            "========================================Predicted Text=================================================== \n",
            "\n",
            "you teal to thin before?'\n",
            "\n",
            "'i don't know what you cone to she tone,\n",
            "' said the ming an snop alice,\n",
            " and she thought it ovt think of the mock turtle was a little before,\n",
            " but the mock turtle said to the mock turtle,\n",
            " so she tere all that she had not artended the words: 'what a catcus-race?' \n",
            "'i don't know what you cone to she tone,\n",
            "' said the ming an snop alice,\n",
            " and she thought it ovt think of the mock turtle was a little before,\n",
            " but the mock turtle said to the mock turtle,\n",
            " so she tere all that she had not artended the words: 'what a catcus-race?' \n",
            "'i don't know what you cone to she tone,\n",
            "' said the ming an snop alice,\n",
            " and she thought it ovt think of the mock turtle was a little before,\n",
            " but the mock turtle said to the mock turtle,\n",
            " so she tere all that she had not artended the words: 'what a catcus-race?' \n",
            "'i don't know what you cone to she tone,\n",
            "' said the ming an snop alice,\n",
            " and she thought it ovt think of the mock turtle was a little before,\n",
            " but the mock turtle said to the mock turtle,\n",
            " so sh\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYayHruGVOVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch 1\n",
        "# epochs 10 batch size = 128\n",
        "# batch 2\n",
        "# epochs 10 batch_size = 64\n",
        "# batch 3\n",
        "# epochs 10 batch_size = 64 filename = improved_3\n",
        "# batch 4\n",
        "# epochs 20 batch_size = 256 filename = improved_4 loss = 1.114\n",
        "# batch 5\n",
        "# epochs 10 batch_size = 256 filename = improved_5 loss = 1.0806\n",
        "# batch 6\n",
        "# epochs 10 batch_size = 256 filename = improved_6 loss = 1.05"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}